# LoLhelper

## The team
- 권영후 (https://k0hoo.github.io/)
- 이성규 (https://seongq.github.io/)
- 채종욱 
- 이상윤 (https://sangyunl.github.io/) 
 
## Problem definition

게임은 현대 사회에서 모든 연령층에서 사랑받는 즐거운 취미로 자리매김하였습니다. 게임 산업의 급속한 성장으로 인해 게임은 단순한 오락 이상의 다양한 의미를 가지게 되었습니다. 그 중에서도 'League of Legends'(이하 롤)은 현재 가장 인기 있는 게임으로 대한민국의 PC방 점유율 1위를 차지하고 있습니다. 

롤은 163개의 챔피언 중 하나를 선택해야 하는데, 이는 초보 사용자들에게는 어려운 과정일 수 있습니다.

적절한 챔피언 선택은 승리를 위한 전략 수립에 주요한 발판이 됩니다. 그러나 챔피언의 수의 증가와 상성 관계로 인해서 초보 유저들이 승리를 위한 적합한 챔피언 선택에 어려움을 겪고 있습니다. 게임 개발자와 커뮤니티는 초보자를 위한 가이드라인, 튜토리얼, 접근성 향상 기능 등을 제공하여 사용자들이 게임을 쉽게 이해하고 적응할 수 있도록 노력하고 있습니다. 그러나 이러한 노력에도 불구하고 여전히 한계가 있습니다. 

이러한 문제를 해결하기 위해 우리는 'LoLhelper'라는 프로그램을 개발하였습니다.

'LoLhelper'는 사용자가 게임 챔피언을 선택할 때 사용자의 전적 정보와 팀원들과 적군의 챔피언 선택을 실시간으로 고려하여 최적/최악의 챔피언을 제시해줍니다. 이를 통해 사용자들이 겪던 챔피언 선택의 어려움을 해소할 수 있을 것입니다.

## System design

### 1) System Flow

![systemflow](./docs_img/KakaoTalk_20230608_124613896_01.png)

1. LOL을 실행하면 프로세스가 이를 감지하여 사용자를 MainPage로 이동시킵니다.
2. 사용자가 로그인하면 LobbyPage로 이동하여 사용자 정보를 보여줍니다.
3. 사용자가 랭크 게임을 실행하면 클라이언트는 LobbyPage에서 Recommend Page로 이동하여 사용자에게 챔피언을 추천합니다.
4. 게임이 시작되거나 게임이 취소될 경우 다시 LobbyPage로 돌아갑니다.
5. LobbyPage에서 LOL 클라이언트 내에서 로그아웃을 선택하면 MainPage로 넘어갑니다.
6. LOL 클라이언트를 종료하면 해당 프로그램도 종료됩니다.

### 2) System Architecture

![systemarchitecture](./docs_img/KakaoTalk_20230608_124613896.png)

프로그램이 RIOT client 및 HPC 서버와 통신하도록 설계되었습니다. 우리의 목표는 게임내의 밴픽현황을 통해서 챔피언을 추천받는 것입니다. 따라서 RIOT client에서 해당 사용자의 정보를 받고 이를 HPC 서버에 전송해서 챔피언을 추천받도록 구성하였습니다. 제안된 시스템에서 HPC 서버는 Flask 프레임워크를 통해서 개발하였으며 HPC 서버내에서 RIOT client로부터 얻은 정보를 토대로 챔피언을 추천하여 전송하게 됩니다. 딥러닝 서버를 별도로 구축함으로써 사용자가 프로그램을 사용하기위한 접근성을 낮췄으며, 사용자로부터 얻은 정보를 수집할 수 있도록 하였습니다.

#### 2-1) Module 1 (Desktop Application ↔ LCU API ↔  RIOT Client)

프로그램의 UI는 **WPF(Windows Presentation Foundation)**로 개발되었습니다. WPF는 윈도우 기반의 응용 프로그램 개발을 위한 기술로, XAML(Extensible Application Markup Language)을 사용하여 사용자 인터페이스를 정의하고 코드 비하인드(C# 또는 VB.NET 등)에서 로직을 구현할 수 있습니다. WPF는 다양한 UI 요소와 그래픽 기능을 제공하며, 사용자 친화적인 인터페이스를 구축하는 데 유용합니다.

WPF를 선택한 이유는 롤 인터페이스 프로세스와의 직접적인 통신을 위해서입니다. RIOT GAMES에서 제공하는 LCU를 통해 롤 클라이언트와 HTTP 양식으로 통신하여 사용자 정보를 주고받습니다. 이러한 작동 방식은 웹과 같은 다른 형태의 UI를 사용하는 것보다 사용자가 프로그램을 더 쉽게 사용할 수 있도록 만들어주며, 프로그램과 클라이언트 간의 내부 프로세스 통신이기 때문에 빠르고 효율적인 통신이 가능합니다.

프로그램은 롤 클라이언트의 작동 여부를 감지하여 클라이언트가 시작되면 사용자를 식별합니다. 프로그램은 클라이언트로부터 사용자의 이번 시즌 전적을 모두 받아온 후, HTTP 응답 데이터를 JSON 객체로 파싱합니다. 개별 챔피언 별로 전적을 정리하고, 그 중에서 판 수가 가장 많은 챔피언부터 정렬하여 저장합니다. 이후 UI에서는 순서대로 가장 많이 사용한 챔피언(Most Five Champion)을 보여줍니다.

랭크 게임 시작 시 챔피언 추천이 시작됩니다. 사용자가 어떤 라인으로 게임을 시작하는지를 클라이언트와의 통신을 통해 파악합니다. 프로그램은 1초마다 챔피언 백/픽 데이터를 반복적으로 받아오며, 챔피언 선택에 변화가 생긴 경우에만 HTTP 응답 데이터를 처리하여 서버에 전달합니다. 이렇게 1초마다 발생하는 통신은 인터프로세스 통신이므로 로드가 적어서 효율적입니다. 로드가 큰 서버와의 통신은 챔피언 선택이 변경될 때만 수행하여 로드를 최소화합니다.

건내주는 데이터에는 사용자의 라인, 금지된 10개의 챔피언, 아군이 선택한 챔피언, 적팀이 선택한 챔피언이 포함됩니다. 라인 정보를 건내주어 서버가 해당 라인에 적합한 챔피언을 추천할 수 있도록 합니다. 이들은 추천 결과를 얻는 입력 데이터로 사용되며, 사용자는 해당 챔피언들을 선택할 수 없는 챔피언으로 간주됩니다.

서버는 추천 챔피언 3개와 비추천 챔피언 2개를 JSON 형식으로 제공하여 실시간으로 프로그램에 전송해줍니다. 프로그램은 이 데이터를 순서대로 UI에 표시합니다. 사용자는 UI에 표시된 추천 챔피언을 클릭함으로써 원하는 챔피언을 선택할 수 있습니다. 프로그램은 이 선택 정보를 HTTP 통신을 통해 서버에 전송합니다. 이를 통해 사용자는 쉽고 빠르게 원하는 챔피언을 선택할 수 있습니다.

#### 2-2) Module 2 (HPC Server)

HPC 서버는 고성능 컴퓨터 서버로서, 딥러닝 모델 제공을 목적으로 사용됩니다. 저희는 Flask 프레임워크를 사용하여 딥러닝 모델을 제공합니다. 모델은 TensorFlow를 사용하여 학습되었습니다. Flask를 선택한 이유는 Python과 TensorFlow와의 호환성을 고려하고, 프로젝트의 규모가 상대적으로 작아서 가볍고 유연한 Flask 프레임워크가 적합하다고 판단했습니다.

클라이언트로부터 전송된 JSON 파일은 HPC 서버에서 전처리 과정을 거치게 됩니다. 전처리는 데이터의 정제, 변환, 스케일링 등을 포함하며, 데이터를 모델이 사용할 수 있는 형식으로 가공하는 작업입니다. 전처리된 데이터는 모델에 입력되어 예측 결과를 반환합니다. 반환된 결과 중에서 상위 3개와 하위 2개를 정렬하여 클라이언트에 다시 전송합니다.

Flask는 요청이 들어온 순서대로 처리되는 단일 스레드 방식을 기본으로 사용합니다. 따라서 다중 사용자가 동시에 HPC 서버에 요청을 보내더라도 순서대로 처리되어 각 사용자에게 모델을 통한 예측 결과가 반환됩니다.

## Data Preparation

### 1) Data Collection

우리는 OP.GG 사이트에서 게임 기록을 수집하기 위해 Selenium을 활용하였습니다. OP.GG에는 다양한 게임 결과가 포함되어 있으며, 이 중에서 저희는 솔로 랭크 게임 데이터만을 수집하였습니다. 일반 게임이나 칼바람 등의 게임 형태는 주로 즐거움을 추구하기 위한 챔피언 선택이 큰 영향을 미치기 때문에, 이러한 게임 기록에서는 승리를 위한 챔피언 선택의 패턴을 파악하기 어려울 것으로 판단되었습니다.

따라서, 저희는 솔로 랭크 게임 데이터를 중점적으로 수집하여 학습에 활용하였습니다. 솔로 랭크 게임은 경쟁적인 환경에서 플레이어들이 최선을 다해 승리를 위해 노력하는 상황을 반영하고 있기 때문에, 게임에서 승리를 위한 챔피언 선택의 패턴을 파악할 수 있을 것으로 기대하였습니다.

그러나, 이러한 데이터 수집 과정에서 몇 가지 문제점이 발생하였습니다. 예를 들어, 일부 게임에서는 데이터가 부족하거나 누락된 경우가 있을 수 있습니다. 또한, OP.GG 사이트의 구조나 업데이트에 따라 데이터 수집에 어려움을 겪는 경우도 있을 수 있습니다. 이러한 문제점들을 인지하고 최대한 해결하기 위해 노력하였으며, 수집된 솔로 랭크 게임 데이터를 효과적으로 활용하여 정확하고 유용한 분석 결과를 도출하는 데 주력하였습니다.

**1. 시간에 따른 데이터의 품질 저하**

LOL은 정기적으로 업데이트가 이루어지며 밸런스 패치를 통해 챔피언의 성능 조절 및 아이템 변경 등이 수정됩니다. 이러한 이유로 과거의 데이터는 현재의 게임 상태와는 더 이상 유의미한 관계를 가지지 않을 수 있습니다. 예를 들어, 이전에 강력한 챔피언으로 알려져 있던 챔피언이 밸런스 패치로 인해 현재에는 그렇게 강력하지 않을 수 있습니다.

우리 팀은 이러한 데이터의 품질 저하 문제를 고려하여 크롤링 일자를 기준으로 최신 7일 이내의 데이터만을 수집하여 학습에 활용하였습니다. 이를 통해 최신 게임 상태에 더 잘 대응하고, 밸런스 조정 등의 변화를 반영한 모델을 구축할 수 있었습니다. 따라서, 우리의 모델은 과거 데이터의 품질 저하로 인한 불필요한 영향을 최소화하고, 현재 게임 상태에 더욱 적합한 예측과 분석을 제공할 수 있습니다.

**2. 라인 스왑**
   
op.gg에서는 게임 기록을 탑, 정글, 미드, 원딜, 서폿 순으로 저장합니다. 그러나 실제 게임에서는 밴픽 단계에서 라인을 스왑하여 원딜 위치에 서포터 챔피언이, 서포터 위치에 원딜 챔피언이 배치되는 경우가 있을 수 있습니다. 이는 데이터 수집 과정에서 잘못된 데이터로 기록될 수 있음을 의미합니다.

우리 팀은 이러한 잘못된 데이터를 처리하는 방법을 고민했습니다. 데이터의 양이 충분하다면 잘못된 데이터는 전체 데이터 중에서 일부이며, 실제 분석 및 학습에 미치는 영향은 제한적일 것으로 예상되었습니다. 따라서, 우리는 해당 데이터를 그대로 수집하여 학습에 활용하기로 결정했습니다. 

이러한 점들을 고려하여서 데이터를 수집하였으며, 결과적으로 저희팀은 챌린저 티어부터 마스터 티어까지 총 5만개의 게임 기록을 수집하였습니다.

### 2) Data Preparation

![crawling1](./docs_img/crawling1.png)

수집한 데이터는 다시 한번 전처리 과정을 통해서 학습에 사용하였습니다. 데이터는 tier, result, enemy1 ~ enemy5, top, jg, mid, ad, sup 순으로 수집되었습니다. 데이터는 승리, 패배 두 가지 경우에 대해서 모두 수집되었기 때문에 학습을 위해서는 승리 결과에 대한 데이터로 만들어줘야했습니다. 따라서 패배한 게임 기록에 대해서는 enemy1 ~ enemy5와 top ~ sup의 챔피언의 정보를 바꿔주어서 패배에 대한 데이터를 승리에 대한 데이터로 변경해주었습니다.

![crawling2](./docs_img/crawling2.png)

우리 팀의 모델의 입력은 적 챔피언 5개, 아군 챔피언 5개의 순으로 벡터를 구성해서 들어가게 됩니다. 따라서 크롤링된 데이터에서 다시 해당 정보만 추출해주었습니다. 그리고 크롤링된 데이터는 챔피언의 이름에 대한 정보로 저장이 되었습니다. 따라서 이를 모델의 학습으로 사용하기 위해서는 챔피언의 이름은 챔피언과 매핑되는 id로 변경해주어서 모델의 입력으로 사용할 수 있게 수정해주었습니다.

![crawling3](./docs_img/crawling3.png)

그리고 모델을 학습하기 위해서는 해당 학습 데이터에 대한 정답 데이터가 쌍으로 존재해야합니다. 자세한 데이터 전처리 결과는 위의 이미지를 참고하세요.


## Model
![model](./docs_img/model_architecture.png)

모델은 선형 레이어를 중첩하여 구현되었으며, 학습 과정은 0 벡터에 적절한 챔피언을 선택하는 것으로 이루어집니다. 입력 벡터에서 빨간색으로 표시된 부분은 적 챔피언의 정보를, 파란색으로 표시된 부분은 아군 챔피언의 정보를 나타냅니다. 적 챔피언 정보는 순서에 상관없이 무작위로 들어가고, 아군 정보는 Top, Jungle, Mid, AD, Sup 순서로 입력 벡터에 할당됩니다. 이 방식으로 벡터의 입력 순서 자체가 적과 아군, 그리고 아군의 역할을 암시하는 정보로 작용합니다.

출력 벡터는 원거리 딜러 포지션을 기준으로 총 26개로 구성되며, 사용자의 포지션에 따라 길이가 변화합니다. 출력 벡터의 순서도 어떤 챔피언을 나타내는지에 대한 정보를 포함하고 있습니다. 예를 들어, 출력 벡터의 4번째 인덱스는 이즈리얼을 표현합니다. 모델은 챔피언의 ID를 학습하는 것이 아니라, 특정 벡터에 1 값을 예측하게 되므로, 이런 방식이 학습의 안정성과 정확도를 향상시키는데 도움이 되었습니다.

초기에 모델을 설계하고 학습할 때, 모델의 손실(loss)이 수렴하지 못하고 발산하는 문제가 발생했습니다. 이 문제를 해결하기 위해 두 가지 방법을 시도했습니다. 첫 번째로, 위에서 언급한대로 출력 벡터(output vector)의 순서 정보를 활용하여 벡터의 값을 챔피언의 id가 아닌 1로 설정했습니다. 이렇게 함으로써 모델이 학습하기 쉽도록 했습니다. 예를 들어, 이전에 1에서 1000 사이의 값을 가졌던 id를 1로 변환했습니다. 두 번째로, 선형 레이어 사이에 배치 정규화(batch normalization) 레이어를 추가하여 값을 너무 크게 만드는 것을 방지했습니다. 이를 통해 값의 스케일을 조절하고, 모델이 안정적으로 학습할 수 있도록 도왔습니다. 위의 두 가지 방법을 통해 모델의 손실이 수렴할 수 있도록 개선했습니다.

출력 레이어의 활성화 함수는 softmax를 사용하였습니다. 분류해야 할 클래스가 다양하므로, 학습 효율을 높이기 위해 softmax 함수를 활용하는 것이 적절하다고 판단했습니다. 실제로, 선형 함수보다는 softmax를 사용한 학습이 더욱 우수한 안정성과 정확도를 보여주었습니다.

## Personalization

모델이 추천해주는 챔피언은 유저의 어떠한 정보도 반영되지 않은 오로지 학습데이터로 인한 결과였습니다. 이러한 챔피언 추천 방식은 유저의 숙련도, 성향 등을 고려하지 않기 때문에 맞춤형 챔피언 추천이라고 할 수 없었습니다. 따라서, 저희 팀은 다음의 식을 도입하여서 우리의 모델을 개인에게 Personalization이 될 수 있도록 하였습니다.

$$ result = \alpha \cdot prediction + (1-\alpha)\cdot \frac {winning\;rate}{max( winning\;rate)}  $$

해당 식에서 result는 최종 결과, prediction은 전처리된 모델의 예측값, alpha는 weight scaling factor의 역할을 합니다. 정규화된 승률은 유저가 플레이한 챔피언들의 승률을 최대 승률로 정규화한 것을 의미합니다. 승률은 10판이상의 플레이한 챔피언들만 반영하여서 적은 판수로 인해서 고승률이 과하게 반영되는 것을 방지하였습니다.

alpha는 prediction의 top3 합으로 구해지게 됩니다. 이때, alpha를 통해서 모델이 예측한 결과 값과 유저의 정보에 해당하는 정규화된 챔피언 별 승률을 조절하게 됩니다. 예측값은 softmax를 한 결과이기 때문에 출력들은 0~1의 값을 갖게 됩니다. 만약 모델이 예측 결과에 확신이 있다면 prediction의 상위 3개의 값의 합은 1에 가까운 값을 갖게 될것입니다. 이러한 경우, 유저 정보에 의한 가중치보다는 모델의 예측한 결과에 가중치를 더 부여해야한다고 판단하였습니다. 그리고 반대의 경우로 전체 출력 값이 비슷하다면 alpha값은 작아지게 될 것입니다. 이러한 경우, 모델이 판단한 결과에 확신이 없다고 해석할 수 있으며 이때는 정규화된 승률에 더 가중치를 주어서 유저 정보를 더 활용해 챔피언을 추천할 수 있도록 하였습니다.


## Application Demonstration


## Feedback after demoday

**1. 중복 챔피언 추천 문제** 

상대팀이 선택한 챔피언을 서버에서 반영하지 못해 이미 선택된 챔피언을 모델이 추천하는 현상이 있었습니다. 이러한 오류는 시스템에 대한 신뢰성을 떨어뜨리게 되며, 이는 사용자의 시스템 만족도 저하로 이어지게 됩니다.
문제를 해결하기 위해서 사용자 클라이언트로부터 밴픽 현황 받고 이를 통해서 모델의 추천 챔피언을 필터링 해줌으로써 해결하였습니다.

**2. Personalization** 

예측된 챔피언이 유저의 챔피언 성향 및 숙련도를 고려하지않고 모델 학습 데이터에 의존적인 결과만 출력한다는 문제점이 있었습니다. 이러한 문제를 위에서 언급한대로 alpha와 정규화된 승률을 도입해서 가중치를 조절하는 방법으로 보완하였습니다. Personalization을 도입한 것으로 기존의 동일한 챔피언 선택상황이 발생하면 동일한 챔피언을 출력해주는 문제에서 사용자의 정보를 반영함으로써 어느정도의 랜덤성도 확보했다고 볼 수 있습니다.





## Reflection

**1. 데이터셋: 업데이트 반영과 지속적인 빠른 학습을 위한 전적 데이터 구성**

롤은 시즌별과 버전별로 업데이트가 활발하게 이루어지며, 게이머들은 RIOT GAMES에서 진행한 업데이트 이외에도 지속적으로 새로운 메타 픽과 전략을 개발합니다. 이러한 새로운 전략들은 커뮤니티를 통해 빠르게 전파됩니다. 롤의 전적 데이터는 이러한 정보들이 지속적으로 누적되며, 기존에 학습된 모델은 최신 정보에 대해 제대로 대응하지 못할 수 있습니다. 그러므로 계속해서 새로운 데이터셋을 구성하고, 레이턴시 데이터의 유효성을 유지하는 것이 중요합니다.


(밑에 문단이 더 나아보여서 수정했습니다. )

롤은 시즌별과 버전별로 업데이트가 활발하게 이루어지며, 게이머들은 RIOT GAMES에서 진행한 업데이트 이외에도 지속적으로 새로운 메타 픽과 전략을 개발합니다. 이러한 새로운 전략들은 커뮤니티를 통해 빠르게 전파됩니다. 롤의 전적 데이터는 이러한 정보들이 지속적으로 누적되며, 기존에 학습된 모델은 최신 정보에 대해 제대로 대응하지 못할 수 있습니다. 그러므로 계속해서 자동적으로 새로운 데이터셋을 구성하고, 유효한 데이터로 모델을 주기적으로 학습할 수 있는 파이프라인을 구축해야할 필요성이 있습니다.

**2. 모델: 정확한 추론 vs 빠른 학습과 결과값 도출**

롤에서는 챔피언 선택부터 게임 플레이와 전략이 하나로 결합되어 승패에 중요한 역할을 합니다. 따라서 최적의 챔피언 선택이라고 해도 게임 내 플레이가 부족하다면 패배할 수 있습니다. 또한 챔피언 선택이 좋지 않다고 해도 게이머가 전략을 이해하고 수립한다면 승리할 수도 있습니다. 이에 따라 챔피언 추천은 어떤 챔피언이 좋을지 경향성을 제시하지만 특정 선택을 강요하지는 않습니다. 따라서 승리의 추천이 가능한지는 확답할 수 없습니다. 하지만 빠른 학습과 빠른 결과 도출은 챔피언 추천에서 매우 중요합니다. 롤은 데이터셋이 빠르게 업데이트되므로 학습과 모델 업데이트가 자주 발생하며, 챔피언 선택 시간 동안 최대 10번의 추천을 제공해야 합니다. 따라서 정확한 추론이 가능한 크고 강력한 모델과 빠르게 작업을 수행할 수 있는 가벼운 모델 중에서 최적의 선택을 해야합니다.

**3. 예상하지 못한 상황에 대한 대처**

게임의 특수성으로 인해 예상치 못한 변화가 발생할 수 있습니다. 예를 들어, 사용자 간에 "line swap"이라는 라인을 바꾸는 행위가 있을 수 있습니다. 이 경우, 모델이 추론한 챔피언이 다른 사용자에게 선택되어 모델이 추천한 챔피언을 사용자가 선택하지 못하는 문제가 발생할 수 있습니다. 다행히도 이러한 문제는 매우 드물고, 특히 낮은 티어에서는 거의 일어나지 않는 현상입니다. 따라서 이에 적절히 대응할 수 있는 방법이 필요합니다. 해결책으로는, 모델을 학습시킬 때 이러한 예상치 못한 변화를 고려하는 것이 중요합니다. 또한, 이러한 상황이 발생할 경우 사용자에게 다른 챔피언을 추천해주는 등의 대응 방안을 마련해야 합니다. 따라서, 게임의 특수성을 고려하여 모델을 학습시키고, 예상치 못한 상황에 대응하는 방법을 고려해야 할 것입니다.


**4. 티어 별 공정성 문제**

챔피언 추천을 받을 사용자와 사용되는 데이터 셋 간에 괴리가 있을 수 있습니다. 처음 롤을 시작하거나 챔피언 선택에 어려움을 겪는 게이머를 위하여 LoLhelper는 챔피언을 추천하여 줍니다. 그렇다면 낮은 티어의 데이터셋을 바탕으로 학습을 하는 것이 맞아보이지만 LoLhelper는 상위 티어의 전적으로 학습되었습니다. 낮은 티어의 데이터에는 챔피언 선택 간에 경향성이 희박하기 때문에 모델이 제대로 학습이 되지 않기 때문입니다. 높은 티어의 게이머는 아군과 적군의 챔피언 선택에 영향을 받아 챔피언을 선택하기 때문에 챔피언 간에 경향성이 분명히 존재합니다.

하지만 높은 티어에서의 챔피언 선택은 수립한 전략을 이행하기 위하여 선택됩니다. 따라서 선택된 챔피언들이 낮은 티어의 게이머들에게서도 제대로 작동할지는 알 수 없는 부분입니다. 롤의 인게임 플레이의 경향성도 파악하여 게이머에게 알려주면 좋겠지만 롤의 인게임 시간은 방대하고 게임 내에서 누적되는 데이터 또한 방대하여 인게임의 전략을 데이터로만 알기는 어려운 부분이 있습니다.

이러한 상황으로 인하여 낮은 티어 혹은 초심자가 만족할 수 있는 챔피언을 추천 받고 추천을 통해 승리를 얻어갈 수 있을지는 알 수 없습니다.

## Future work

우리는 더 나은 서비스를 제공하기 위해 룬 특성 추천 시스템을 구현할 계획입니다. OP.GG 사이트에서 승리한 게임의 룬 특성 데이터를 분석하여, 사용자가 선택한 챔피언에 맞는 룬 특성을 추천해줄 것입니다. 또한, 게임 시즌이 변경될 때마다 다양한 메타가 등장하는 게임에서 시즌 별 데이터를 수집하여 자동으로 반영할 예정입니다. 이를 통해 사용자가 게임을 보다 효율적으로 즐길 수 있도록 끊임없이 피드백을 받아 완성도 높은 프로그램을 만들어 나갈 것입니다.